{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\title{Gaussian Assumption Over Linear Regression}\n",
    "\\author{Dristanta Das}\n",
    "${February} {2021}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\section{Introduction}\n",
    "Linear regression attempts to model the relationship between two variables by fitting a line to the observed data. One variable is considered to be the independent variable and the other is considered to be the dependent variable.\n",
    "\n",
    "\\section{Probabilistic Modelling}\n",
    "\n",
    "\\subsection{Linear Model}\n",
    "\n",
    "\n",
    "\n",
    "\\qquad $y_{i} \\simeq \\theta^T x_{i}$ \\\\\n",
    "   \n",
    " $y_{i} = \\theta^T x_{i} + \\epsilon_{i}$ , where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2}) $ \n",
    "\n",
    "Here $\\epsilon_{i}$'s are i.i.d random variables. \\\\\n",
    "\n",
    "\n",
    "$\\epsilon_{i} \\sim \\mathcal{N} (0,\\sigma^{2})$ \n",
    "    \n",
    "$\\Rightarrow p(\\epsilon_{i}) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\left (^ {-\\frac{\\epsilon_{i}^2}{2 \\sigma^2}}\\right)$ \\\\\n",
    "    \n",
    "$\\Rightarrow p(y_{i}-\\theta^T x_{i}) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\left (^ {-\\frac{(y_{i}-\\theta^T x_{i})^2}{2 \\sigma^2}}\\right)$ \\\\\n",
    "    \n",
    "However the conventional way is, \\\\\n",
    "    \n",
    "$p(y_{i}|x_{i};\\theta) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp \\left (^ {-\\frac{(y_{i}-\\theta^T x_{i})^2}{2 \\sigma^2}}\\right)$ \\\\\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\subsection{Parameter Estimation}\n",
    "\n",
    "Suppose, we are given with a dataset, $\\mathcal{D}$  = { $x_{i}, y_{i}$}$_{i=1}^{m}$ \\\\\n",
    "\n",
    "Then, Bayes' Theorem States that,\n",
    "\n",
    "        \n",
    "$P(\\theta | \\mathcal{D} )  = P(\\mathcal{D} | \\theta) . P(\\theta)$ \\\\\n",
    "    \n",
    "$\\qquad  = P(\\theta , \\mathcal{D})\\frac{1}{P(\\mathcal{D})}$ \\\\\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T07:47:11.824350Z",
     "start_time": "2021-02-28T07:47:11.793511Z"
    }
   },
   "source": [
    "\\subsection{Maximum Likelihood Estimation (MLE)}\n",
    "\n",
    "The idea behind maximum likelihood estimation (MLE) is to define a function of the parameters that enables us to find a model that fits the data\n",
    "well. The estimation problem is focused on the likelihood function, or\n",
    "more precisely its negative logarithm. For data represented by a random\n",
    "variable x and for a family of probability densities $p( x   |  \\theta )$ parametrized\n",
    "by $\\theta$ , the negative log-likelihood is given by\n",
    "$$\\mathcal{L}_{x}(\\theta) = - log p( x  |  \\theta)$$\n",
    "The notation $\\mathcal{L}_{x}(\\theta)$ emphasizes the fact that the parameter $\\theta$ is varying\n",
    "and the data x is fixed. We very often drop the reference to x when writing\n",
    "the negative log-likelihood, as it is really a function of $\\theta$ , and write it as\n",
    "$\\mathcal{L}(\\theta)$ when the random variable representing the uncertainty in the data\n",
    "is clear from the context.\n",
    "Let us interpret what the probability density $p( x  |  \\theta )$ is modeling for a\n",
    "fixed value of $\\theta$ . It is a distribution that models the uncertainty of the data.\n",
    "In other words, once we have chosen the type of function we want as a\n",
    "predictor, the likelihood provides the probability of observing data x .\n",
    "In a complementary view, if we consider the data to be fixed (because\n",
    "it has been observed), and we vary the parameters $\\theta$ , what does $\\mathcal{L}(\\theta)$ tell us? It tells us how likely a particular setting of $\\theta$ is for the observations x.Based on this second view, the maximum likelihood estimator gives us the most likely parameter $\\theta$ for the set of data.\n",
    "We consider the supervised learning setting, where we obtain pairs\n",
    "$(x_{1},y_{1})$, . . . , $(x_{N},y_{N})$ with   and labels $y_{N} \\in \\mathbb{R}^n$. We are interested in constructing a predictor that takes a feature vector $x_{n}$ as input and produces a prediction y n (or something close to it), i.e., given a vector $x_{n}$ we want the probability distribution of the label $y_{n}$ . In other words,\n",
    "we specify the conditional probability distribution of the labels given the\n",
    "examples for the particular parameter setting $\\theta$ .\\\\\n",
    "We assume that the set of examples $(x_{1},y_{1})$, . . . , $(x_{N},y_{N})$ are independent\n",
    "and identically distributed (i.i.d.). The word “independent” \n",
    "implies that the likelihood of the whole dataset  Y = {{$y_{1} , . . . , y_{N} $}} and\n",
    "X = {{$x_{1} , . . . , x_{N} $}} factorizes into a product of the likelihoods of each individual example,\n",
    "$$p(Y|X, \\theta) = \\prod_{n=1}^{N}p(y_{n}|x_{n}, \\theta)$$\n",
    "\n",
    "where $p(y_{n} | x_{n} , \\theta)$ is a particular distribution (which was Gaussian). The expression “identically distributed” means that each term\n",
    "in the product, is of the same distribution, and all of them share\n",
    "the same parameters. It is often easier from an optimization viewpoint to\n",
    "compute functions that can be decomposed into sums of simpler functions.\n",
    "Hence, in machine learning we often consider the negative log-likelihood,\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -log\\  p(Y|X, \\theta) = - \\sum_{n=1}^{N}log\\  p(y_{n}|x_{n}, \\theta)$$\n",
    "\n",
    "While it is temping to interpret the fact that $\\theta$ is on the right of the condi-\n",
    "tioning in $p(y_{n} | x_{n} , \\theta)$, and hence should be interpreted as observed\n",
    "and fixed, this interpretation is incorrect. The negative log-likelihood $\\mathcal{L}(\\theta)$\n",
    "is a function of $\\theta$ . Therefore, to find a good parameter vector $\\theta$ that\n",
    "explains the data $(x_{1},y_{1})$, . . . , $(x_{N},y_{N})$ well, minimize the negative log-\n",
    "likelihood $\\mathcal{L}(\\theta)$ with respect to $\\theta$ .\\\\\n",
    "\n",
    "Remark. The negative sign  is a historical artifact that is due\n",
    "to the convention that we want to maximize likelihood, but numerical\n",
    "optimization literature tends to study minimization of functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:38:15.228541Z",
     "start_time": "2021-02-25T16:38:15.209163Z"
    }
   },
   "source": [
    "\n",
    "We will make a Gaussian model assumption with $\\theta \\in \\mathbb{R}^n$  \n",
    "\n",
    "  \n",
    "\n",
    "$\\theta^*  = argmax_\\theta  \\mathcal{L} (\\theta | \\mathcal{D})$ \\\\\n",
    "\n",
    "$= argmax_\\theta P(\\mathcal{D}  |  \\theta))$ \\\\\n",
    "         \n",
    "$= argmax_\\theta P(y_{1},x_{1},...,y_{m},x_{m};\\theta)$ \\\\\n",
    "        \n",
    "$= argmax_\\theta \\prod_{i = 1}^{m} P(y_{i},x_{i} ; \\theta)$ \\\\\n",
    "          \n",
    "$= argmax_\\theta \\prod_{i = 1}^{m} [P(y_{i}|x_{i} ; \\theta). P(x_{i}; \\theta ]$ \\\\\n",
    "          \n",
    "$= argmax_\\theta \\prod_{i = 1}^{m} [P(y_{i}| x_{i} ; \\theta)] . P(x_{i})$\\\\\n",
    "          \n",
    "$= argmax_\\theta \\prod_{i = 1}^{m} [P(y_{i}| x_{i} ; \\theta)]$\\\\\n",
    "          \n",
    "$= argmax_\\theta\\sum_{i = 1}^{m}  log P(y_{i} | x_{i} ; \\theta )$ \\\\\n",
    "          \n",
    "$= argmax_\\theta\\sum_{i=1}^{m} [log(\\frac{1}{\\sqrt{2 \\pi}\\sigma}) + log(exp(^-\\frac{(\\theta^T x_{i} - y_{i})^2}{2 \\sigma^2}))]$\\\\\n",
    "          \n",
    "$= argmax_\\theta-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^{m} (\\theta^T x_{i} - y_{i})^2$ \\\\\n",
    "          \n",
    "$= argmax_\\theta\\frac{1}{n}\\quad\\sum_{i=1}^{m} (\\theta^T x_{i} - y_{i})^2$ \\\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Conclusion}\n",
    "Hence, Under Gaussian assumption linear regression amounts to least square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Gaussian Assumption Over Linear Regression",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
